{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 6 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Decision Tree Classifier\n",
    "\n",
    "This Assignment is mainly focused on using decision trees to classify instances. As such, our first step is to create a function to generate these decision trees.\n",
    "\n",
    "First, we will need to import several modules later in this code, so we will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import copy\n",
    "import random\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will need several helper functions that will be used in the `tdidt()` classifier.\n",
    "\n",
    "The first set are functions that are copied from the functions developed in class. We will pull `group_by()` and `get_column()` from previous repositories, as well as `partition_instances()` from the starter code for this assignment located in the `DecisionTreeFun` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Gina's Repo\n",
    "def groupBy(table, column_index, include_only_column_index=None):\n",
    "    # first identify unique values in the column\n",
    "    group_names = sorted(list(set(get_column(table, column_index))))\n",
    "\n",
    "    # now, we need a list of subtables\n",
    "    # each subtable corresponds to a value in group_names\n",
    "    # parallel arrays\n",
    "    groups = [[] for name in group_names]\n",
    "    for row in table:\n",
    "        # which group does it belong to?\n",
    "        group_by_value = row[column_index]\n",
    "        index = group_names.index(group_by_value)\n",
    "        if include_only_column_index is None:\n",
    "            groups[index].append(row.copy()) # note: shallow copy\n",
    "        else:\n",
    "            groups[index].append(row[include_only_column_index])\n",
    "\n",
    "    return group_names, groups\n",
    "\n",
    "# takes a table and a column index\n",
    "# returns a column at index where values are converted to numeric\n",
    "def get_column(table, column_index):\n",
    "    column = []\n",
    "    for item in table:\n",
    "        column.append(item[column_index])\n",
    "    return column\n",
    "\n",
    "def partition_instances(instances, att_index, att_domain):\n",
    "    # this is a group by att_domain, not by att_values in instances\n",
    "    partition = {}\n",
    "    for att_value in att_domain:\n",
    "        subinstances = []\n",
    "        for instance in instances:\n",
    "            # check if this instance has att_value at att_index\n",
    "            if instance[att_index] == att_value:\n",
    "                subinstances.append(instance)\n",
    "        partition[att_value] = subinstances\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to develop several helper functions of our own to use. These are defined below:\n",
    "\n",
    "* `check_all_same_att()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances to check\n",
    "        * `index` - The index to query\n",
    "    * **Returns**:\n",
    "        * `True` if all instances in `instances` have the same value at `index`; `False` if else.\n",
    "* `check_all_same_class()`\n",
    "     * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances to check\n",
    "        * `class_index` - The index of the classifying variable\n",
    "    * **Returns**:\n",
    "        * `True` if all instances in `instances` have the same class; `False` if else.\n",
    "\n",
    "* `select_attribute()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances\n",
    "        * `att_indexes` - A list of valid indices to split on\n",
    "        * `class_index` - The index of the classifying attribute\n",
    "    * **Returns**:\n",
    "        * Uses the calculations for Entropy and Information Gain discussed in class to return the index attribute with the lowest entropy to be split on next\n",
    "* `handle_clash()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances\n",
    "        * `class_index` - The index of the classifying attribute\n",
    "    * **Returns**:\n",
    "        * Uses majority voting to resolve clashes in the decision tree and create a Leaf Node with the most frequent class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_same_att(instances, index):\n",
    "    base = instances[0][index]\n",
    "    for elem in instances:\n",
    "        if elem[index] != base:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_all_same_class(instances, class_index):\n",
    "    base = instances[0][class_index]\n",
    "    for elem in instances:\n",
    "        if elem[class_index] != base:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def select_attribute(instances, att_indexes, class_index):\n",
    "    Entropy_list = {}\n",
    "    for index in att_indexes:\n",
    "        E_new = 0\n",
    "        names, values = groupBy(instances, index)\n",
    "        for val in values:\n",
    "            ratios = {}\n",
    "            total = 0\n",
    "            for instance in val:\n",
    "                if instance[class_index] not in ratios:\n",
    "                    ratios[instance[class_index]] = 1\n",
    "                else:\n",
    "                    ratios[instance[class_index]] += 1\n",
    "                total += 1\n",
    "            E = 0\n",
    "            for ratio in ratios:\n",
    "                E += (ratios[ratio] / total) * math.log((ratios[ratio] / total), 2)\n",
    "            E_new += (total / len(instances)) * -E\n",
    "        Entropy_list[index] = E_new\n",
    "\n",
    "    min_i = att_indexes[0]\n",
    "    for index in att_indexes:\n",
    "        if Entropy_list[index] < Entropy_list[min_i]:\n",
    "            min_i = index\n",
    "    return min_i\n",
    "\n",
    "def handle_clash(instances, class_index):\n",
    "    votes = {}\n",
    "    for instance in instances:\n",
    "        if instance[class_index] not in votes:\n",
    "            votes[instance[class_index]] = 1\n",
    "        else:\n",
    "            votes[instance[class_index]] += 1\n",
    "    # Referenced from https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    sorted_x = sorted(votes.items(), reverse=True, key=operator.itemgetter(1))\n",
    "    return [\"Leaf\", sorted_x[0][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these classifiers, we can construct our `tdidt()` decision tree generator, as well as the `classify_tdidt()` classifier.\n",
    "\n",
    "* `tdidt()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - The currently partitioned instances. On first recursive call, these are initialized as the entire dataset.\n",
    "        * `att_indexes` - A list of all valid indices to split on. On first recursive call, these are initialized as all attribute indices.\n",
    "        * `att_domains` - A list of all valid values for each attribute in the dataset.\n",
    "        * `class_index` - The index of the classifying attribute.\n",
    "    * **Returns**:\n",
    "        * A Decision Tree, represented using nested lists\n",
    "* `classify_tdidt()`\n",
    "    * **Parameters**:\n",
    "        * `tree` - A decision tree, generated by `tdidt()`\n",
    "        * `instance` - The unseen instance to classify\n",
    "    * **Returns**:\n",
    "        * A predicted classification using the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdidt(instances, att_indexes, att_domains, class_index):\n",
    "    if check_all_same_class(instances, class_index):\n",
    "        return [\"Leaf\", instances[0][class_index]]\n",
    "    if att_indexes == []:\n",
    "        return handle_clash(instances, class_index)\n",
    "    index = select_attribute(instances, att_indexes, class_index)\n",
    "    new_indexes = att_indexes[:]\n",
    "    new_indexes.remove(index)\n",
    "    if check_all_same_att(instances, index):\n",
    "        return tdidt(instances, new_indexes, att_domains, class_index)\n",
    "    else:\n",
    "        tree = [\"Attribute\", index]\n",
    "        partitions = partition_instances(instances, index, att_domains[index])\n",
    "        for val in partitions:\n",
    "            if (partitions[val] == []):\n",
    "                return handle_clash(instances, class_index)\n",
    "            tree.append([\"Value\", val, tdidt(partitions[val], new_indexes, att_domains, class_index)])\n",
    "        return tree\n",
    "\n",
    "def classify_tdidt(tree, instance):\n",
    "    if tree[0] == 'Leaf':\n",
    "        return tree[1]\n",
    "    else:\n",
    "        i = 2\n",
    "        while (instance[tree[1]] != tree[i][1]):\n",
    "            i += 1\n",
    "        return classify_tdidt(tree[i][2], instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our classifier, we will make use of the \"interview\" dataset provided in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\", \"False\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first generate a decision tree using `tdidt()`, and then we will classify two instances: (\"Senior\", \"Java\", \"no\", \"yes\"), which exists in the dataset and should have the classification (\"False\"); and (\"Junior\", \"Java\", \"no\", \"no\"), which is not in the dataset but based on our in-class tree, should have the classification (\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tree = tdidt(table, [0, 1, 2, 3], [[\"Senior\", \"Mid\", \"Junior\"], [\"Java\", \"Python\", \"R\"], [\"no\", \"yes\"], [\"no\", \"yes\"]], 4)\n",
    "\n",
    "print(classify_tdidt(tree, [\"Senior\", \"Java\", \"no\", \"yes\"]))\n",
    "print(classify_tdidt(tree, [\"Junior\", \"Java\", \"no\", \"no\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Titanic Data\n",
    "\n",
    "This step asks us to use our classifier for the titanic dataset, and to test it using stratified 10-fold cross-validation.\n",
    "\n",
    "To do this, the first step will be to import the dataset and clean it as we have in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def numerify_instance(instance):\n",
    "    new = []\n",
    "    for attribute in instance:\n",
    "        try:\n",
    "            new.append(float(attribute))\n",
    "        except:\n",
    "            new.append(attribute)\n",
    "    return new\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset_r = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset_r.append(instance)\n",
    "    dataset = []\n",
    "    for instance in dataset_r:\n",
    "        newInstance = numerify_instance(instance)\n",
    "        dataset.append(newInstance)\n",
    "    return dataset\n",
    "\n",
    "def resolve_missing_values(data):\n",
    "    for i in range(10):\n",
    "        if i != 8:\n",
    "            sum_i = 0\n",
    "            count_i = 0\n",
    "            for instance in data:\n",
    "                if instance[i] != \"NA\":\n",
    "                    try:\n",
    "                        sum_i += instance[i]\n",
    "                        count_i += 1\n",
    "                    except:\n",
    "                        print(instance[i])\n",
    "            if count_i == 0:\n",
    "                continue\n",
    "            mean = sum_i / count_i\n",
    "            for instance in data:\n",
    "                if instance[i] == \"NA\":\n",
    "                    instance[i] = mean\n",
    "                    \n",
    "titanic_data = create_dataset(read_data(\"titanic_data.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reuse the `create_cross_fold()` function from PA4 and PA5 for creating subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_fold(data, n):\n",
    "    data_r = copy.deepcopy(data)\n",
    "    random.shuffle(data_r)\n",
    "    size = int(len(data) * 1/n) \n",
    "    start = 0\n",
    "    end = size\n",
    "    folds = []\n",
    "    for i in range(n-1):\n",
    "        folds.append(data[start:end])\n",
    "        start = end + 1\n",
    "        end += size + 1\n",
    "    folds.append(data[start:])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do our cross-fold testing. We will write a function, `confusion_matrix_tdidt()` to both do the classification and generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_titanic(data, att_indexes, att_domains, class_index, n):\n",
    "    results = [[\"no\", 0, 0], [\"yes\", 0, 0]]\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        curr_tree = tdidt(train, att_indexes, att_domains, class_index)\n",
    "        predictions = [classify_tdidt(curr_tree, instance[:3]) for instance in test]\n",
    "        actual = [instance[3] for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == \"no\":\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[0][1] += 1\n",
    "                else:\n",
    "                    results[0][2] += 1\n",
    "            else:\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[1][1] += 1\n",
    "                else:\n",
    "                    results[1][2] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row[1:]))\n",
    "        if row[0] == \"no\":\n",
    "            row.append((row[1] / row[3]) * 100)\n",
    "        else:\n",
    "            row.append((row[2] / row[3]) * 100)\n",
    "    header = [\"Survived\", \"no\", \"yes\", \"Total\", \"Recognition (%)\"]\n",
    "    print(\"Decision Tree (Stratefied \", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do our classification and generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Stratefied 10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "Survived      no    yes    Total    Recognition (%)\n",
      "----------  ----  -----  -------  -----------------\n",
      "no           268    440      708            37.8531\n",
      "yes           20   1464     1484            98.6523\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_titanic(titanic_data, [0, 1, 2], [[\"crew\", \"third\", \"second\", \"first\"], [\"adult\", \"child\"], [\"male\", \"female\"], [\"yes\", \"no\"]], 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MPG Classification\n",
    "\n",
    "For this step, we will use our `tdidt()` decision tree to classify mpg data from the auto dataset. As before, our first step is to import and clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data_r = create_dataset(read_data(\"auto-data.txt\"))\n",
    "resolve_missing_values(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will restrict what attributes are included in our dataset. For the purpose of this assignment, we are concerned with the cylinders, weight, and model year attributes, as well as mpg as a classifier. We will copy `clean_auto_data()` from PA5 to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_auto_data(data):\n",
    "    cleaned_auto_data = []\n",
    "    for instance in data:\n",
    "        cleaned_instance = [instance[1], instance[4], instance[6], instance[0]]\n",
    "        cleaned_auto_data.append(cleaned_instance)\n",
    "    return cleaned_auto_data\n",
    "\n",
    "auto_data_c = clean_auto_data(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as we did in PA5, we need to discretize the continuous attributes weight and cylinder. We will do this using the NHSTA vehicle size classification table and the DOE mpg classification rating, provided below.\n",
    "\n",
    "| Rating | MPG   |\n",
    "|--------|-----  |\n",
    "|   10   | ≥ 45  |\n",
    "|   9    | 37-44 |\n",
    "|   8    | 31-36 |\n",
    "|   7    | 27-30 |\n",
    "|   6    | 24-26 |\n",
    "|   5    | 20-23 |\n",
    "|   4    | 17-19 |\n",
    "|   3    | 15-16 |\n",
    "|   2    |   14  |\n",
    "|   1    | ≤ 13  |\n",
    "\n",
    "| Ranking |  Weight   |\n",
    "|---------|-----------|\n",
    "|    5    | ≥ 3500    |\n",
    "|    4    | 3000-3499 |\n",
    "|    3    | 2500-2999 |\n",
    "|    2    | 2000-2499 |\n",
    "|    1    | ≤ 1999    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   [8.0, 5, 70.0, 3],\n",
      "    [8.0, 5, 70.0, 5],\n",
      "    [8.0, 4, 70.0, 2],\n",
      "    [8.0, 5, 70.0, 3],\n",
      "    [8.0, 5, 70.0, 5],\n",
      "    [8.0, 5, 70.0, 4],\n",
      "    [8.0, 5, 70.0, 2],\n",
      "    [8.0, 5, 70.0, 3],\n",
      "    [8.0, 5, 70.0, 3],\n",
      "    [8.0, 5, 70.0, 3],\n",
      "    [6.0, 3, 70.0, 5],\n",
      "    [8.0, 4, 70.0, 5],\n",
      "    [8.0, 4, 70.0, 4],\n",
      "    [8.0, 5, 70.0, 5],\n",
      "    [6.0, 3, 70.0, 5],\n",
      "    [8.0, 5, 70.0, 2],\n",
      "    [8.0, 4, 70.0, 4],\n",
      "    [8.0, 5, 70.0, 5],\n",
      "    [8.0, 5, 70.0, 2],\n",
      "    [4.0, 2, 70.0, 6],\n",
      "    [6.0, 3, 71.0, 4],\n",
      "    [6.0, 3, 71.0, 4],\n",
      "    [6.0, 4, 71.0, 4],\n",
      "    [4.0, 2, 70.0, 6],\n",
      "    [4.0, 2, 70.0, 6],\n",
      "    [8.0, 5, 71.0, 2],\n",
      "    [4.0, 2, 71.0, 5],\n",
      "    [4.0, 2, 71.0, 7],\n",
      "    [8.0, 5, 70.0, 1],\n",
      "    [8.0, 5, 70.0, 1],\n",
      "    [8.0, 5, 71.0, 1],\n",
      "    [4.0, 2, 71.0, 7],\n",
      "    [8.0, 5, 71.0, 1],\n",
      "    [8.0, 5, 70.0, 1],\n",
      "    [6.0, 4, 71.0, 4],\n",
      "    [4.0, 2, 71.0, 6],\n",
      "    [6.0, 4, 71.0, 4],\n",
      "    [4.0, 2, 71.0, 7],\n",
      "    [4.0, 3, 70.0, 6],\n",
      "    [4.0, 1, 71.0, 6],\n",
      "    [8.0, 5, 71.0, 2],\n",
      "    [6.0, 4, 71.0, 3],\n",
      "    [8.0, 5, 71.0, 2],\n",
      "    [6.0, 4, 71.0, 4],\n",
      "    [8.0, 5, 71.0, 1],\n",
      "    [4.0, 2, 70.0, 6],\n",
      "    [4.0, 1, 71.0, 8],\n",
      "    [4.0, 2, 71.0, 6],\n",
      "    [8.0, 5, 72.0, 4],\n",
      "    [8.0, 5, 72.0, 3],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [4.0, 2, 72.0, 5],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [4.0, 2, 72.0, 7],\n",
      "    [4.0, 2, 72.0, 6],\n",
      "    [4.0, 2, 72.0, 5],\n",
      "    [4.0, 2, 72.0, 5],\n",
      "    [3.0, 2, 72.0, 4],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [4.0, 3, 72.0, 5],\n",
      "    [8.0, 5, 72.0, 3],\n",
      "    [8.0, 5, 72.0, 2],\n",
      "    [8.0, 5, 72.0, 2],\n",
      "    [4.0, 2, 72.0, 6],\n",
      "    [4.0, 2, 72.0, 7],\n",
      "    [4.0, 2, 72.0, 6],\n",
      "    [4.0, 3, 72.0, 5],\n",
      "    [4.0, 3, 72.0, 5],\n",
      "    [4.0, 2, 72.0, 5],\n",
      "    [4.0, 3, 72.0, 4],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [6.0, 3, 73.0, 4],\n",
      "    [6.0, 3, 73.0, 4],\n",
      "    [8.0, 5, 73.0, 2],\n",
      "    [4.0, 3, 73.0, 5],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 3],\n",
      "    [6.0, 4, 73.0, 3],\n",
      "    [4.0, 2, 73.0, 5],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 3],\n",
      "    [8.0, 4, 73.0, 3],\n",
      "    [4.0, 2, 73.0, 6],\n",
      "    [4.0, 1, 73.0, 7],\n",
      "    [8.0, 5, 73.0, 2],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [6.0, 4, 73.0, 4],\n",
      "    [4.0, 2, 73.0, 4],\n",
      "    [3.0, 2, 73.0, 4],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 2],\n",
      "    [6.0, 4, 73.0, 4],\n",
      "    [8.0, 5, 73.0, 3],\n",
      "    [4.0, 3, 73.0, 6],\n",
      "    [4.0, 2, 73.0, 5],\n",
      "    [4.0, 1, 73.0, 6],\n",
      "    [4.0, 3, 73.0, 4],\n",
      "    [6.0, 3, 74.0, 4],\n",
      "    [6.0, 5, 74.0, 3],\n",
      "    [8.0, 5, 74.0, 2],\n",
      "    [4.0, 2, 74.0, 7],\n",
      "    [8.0, 5, 74.0, 1],\n",
      "    [6.0, 4, 74.0, 3],\n",
      "    [4.0, 3, 74.0, 6],\n",
      "    [4.0, 2, 74.0, 7],\n",
      "    [8.0, 5, 74.0, 2],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [4.0, 2, 74.0, 8],\n",
      "    [8.0, 5, 74.0, 3],\n",
      "    [8.0, 5, 74.0, 2],\n",
      "    [6.0, 3, 74.0, 5],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [6.0, 4, 74.0, 5],\n",
      "    [6.0, 5, 74.0, 4],\n",
      "    [4.0, 2, 74.0, 6],\n",
      "    [4.0, 1, 74.0, 8],\n",
      "    [4.0, 1, 74.0, 8],\n",
      "    [4.0, 1, 74.0, 6],\n",
      "    [6.0, 3, 75.0, 5],\n",
      "    [6.0, 5, 75.0, 3],\n",
      "    [6.0, 4, 75.0, 4],\n",
      "    [4.0, 3, 75.0, 5],\n",
      "    [6.0, 5, 75.0, 4],\n",
      "    [6.0, 4, 75.0, 5],\n",
      "    [8.0, 5, 75.0, 3],\n",
      "    [8.0, 4, 75.0, 5],\n",
      "    [6.0, 4, 75.0, 4],\n",
      "    [8.0, 5, 75.0, 2],\n",
      "    [6.0, 4, 75.0, 3],\n",
      "    [8.0, 4, 75.0, 1],\n",
      "    [4.0, 3, 75.0, 5],\n",
      "    [4.0, 1, 75.0, 8],\n",
      "    [6.0, 4, 75.0, 3],\n",
      "    [4.0, 3, 75.0, 5],\n",
      "    [6.0, 5, 75.0, 4],\n",
      "    [8.0, 5, 75.0, 3],\n",
      "    [6.0, 4, 75.0, 4],\n",
      "    [4.0, 3, 75.0, 5],\n",
      "    [8.0, 5, 75.0, 3],\n",
      "    [4.0, 3, 75.0, 6],\n",
      "    [4.0, 2, 75.0, 7],\n",
      "    [4.0, 3, 75.0, 6],\n",
      "    [4.0, 2, 75.0, 6],\n",
      "    [4.0, 1, 75.0, 7],\n",
      "    [4.0, 3, 75.0, 5],\n",
      "    [6.0, 4, 76.0, 5],\n",
      "    [8.0, 5, 76.0, 3],\n",
      "    [6.0, 4, 76.0, 4],\n",
      "    [8.0, 5, 76.0, 3],\n",
      "    [8.0, 5, 76.0, 4],\n",
      "    [4.0, 2, 76.0, 7],\n",
      "    [6.0, 4, 76.0, 5],\n",
      "    [8.0, 5, 76.0, 1],\n",
      "    [6.0, 5, 76.0, 5],\n",
      "    [4.0, 2, 76.0, 6],\n",
      "    [8.0, 5, 76.0, 3],\n",
      "    [8.0, 5, 76.0, 1],\n",
      "    [4.0, 2, 76.0, 7],\n",
      "    [6.0, 5, 76.0, 4],\n",
      "    [6.0, 4, 76.0, 6],\n",
      "    [4.0, 3, 76.0, 6],\n",
      "    [4.0, 1, 76.0, 8],\n",
      "    [6.0, 5, 76.0, 3],\n",
      "    [4.0, 4, 76.0, 4],\n",
      "    [6.0, 4, 76.0, 5],\n",
      "    [8.0, 5, 76.0, 1],\n",
      "    [6.0, 5, 76.0, 4],\n",
      "    [4.0, 2, 76.0, 7],\n",
      "    [4.0, 2, 76.0, 7],\n",
      "    [4.0, 1, 76.0, 7],\n",
      "    [4.0, 4, 76.0, 5],\n",
      "    [4.0, 1, 76.0, 7],\n",
      "    [4.0, 3, 77.0, 5],\n",
      "    [6.0, 4, 77.0, 5],\n",
      "    [8.0, 5, 77.0, 4],\n",
      "    [4.0, 2, 77.0, 7],\n",
      "    [6.0, 5, 77.0, 4],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [4.0, 2, 77.0, 8],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [6.0, 5, 77.0, 4],\n",
      "    [4.0, 3, 77.0, 6],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [4.0, 2, 77.0, 8],\n",
      "    [3.0, 3, 77.0, 5],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [8.0, 5, 77.0, 4],\n",
      "    [4.0, 2, 77.0, 6],\n",
      "    [6.0, 5, 77.0, 4],\n",
      "    [8.0, 5, 77.0, 3],\n",
      "    [4.0, 3, 77.0, 6],\n",
      "    [4.0, 1, 77.0, 8],\n",
      "    [4.0, 1, 77.0, 7],\n",
      "    [4.0, 2, 77.0, 6],\n",
      "    [4.0, 2, 77.0, 7],\n",
      "    [4.0, 1, 77.0, 7],\n",
      "    [6.0, 4, 78.0, 4],\n",
      "    [6.0, 4, 78.0, 4],\n",
      "    [5.0, 3, 78.0, 5],\n",
      "    [6.0, 4, 78.0, 5],\n",
      "    [6.0, 4, 78.0, 4],\n",
      "    [4.0, 2, 78.0, 7],\n",
      "    [6.0, 4, 78.0, 5],\n",
      "    [8.0, 4, 78.0, 4],\n",
      "    [6.0, 5, 78.0, 4],\n",
      "    [8.0, 5, 78.0, 4],\n",
      "    [8.0, 5, 78.0, 4],\n",
      "    [4.0, 2, 78.0, 7],\n",
      "    [6.0, 3, 78.0, 5],\n",
      "    [4.0, 3, 78.0, 6],\n",
      "    [4.0, 2, 78.0, 7],\n",
      "    [4.0, 1, 78.0, 8],\n",
      "    [4.0, 1, 78.0, 8],\n",
      "    [8.0, 5, 78.0, 5],\n",
      "    [6.0, 4, 78.0, 5],\n",
      "    [8.0, 4, 78.0, 4],\n",
      "    [4.0, 3, 78.0, 5],\n",
      "    [6.0, 4, 78.0, 3],\n",
      "    [4.0, 3, 78.0, 5],\n",
      "    [6.0, 4, 78.0, 5],\n",
      "    [6.0, 5, 78.0, 4],\n",
      "    [4.0, 3, 78.0, 5],\n",
      "    [4.0, 3, 78.0, 5],\n",
      "    [4.0, 3, 78.0, 7],\n",
      "    [4.0, 1, 78.0, 9],\n",
      "    [4.0, 1, 78.0, 8],\n",
      "    [6.0, 4, 78.0, 4],\n",
      "    [6.0, 4, 79.0, 5],\n",
      "    [4.0, 3, 79.0, 7],\n",
      "    [8.0, 5, 79.0, 3],\n",
      "    [4.0, 3, 79.0, 7],\n",
      "    [8.0, 5, 79.0, 5],\n",
      "    [8.0, 5, 79.0, 4],\n",
      "    [8.0, 5, 79.0, 4],\n",
      "    [8.0, 5, 79.0, 4],\n",
      "    [6.0, 4, 79.0, 5],\n",
      "    [4.0, 1, 79.0, 8],\n",
      "    [8.0, 5, 79.0, 4],\n",
      "    [4.0, 2, 79.0, 9],\n",
      "    [8.0, 5, 79.0, 3],\n",
      "    [4.0, 3, 79.0, 5],\n",
      "    [8.0, 5, 79.0, 4],\n",
      "    [4.0, 1, 79.0, 8],\n",
      "    [5.0, 5, 79.0, 6],\n",
      "    [8.0, 5, 79.0, 3],\n",
      "    [6.0, 3, 79.0, 4],\n",
      "    [8.0, 4, 79.0, 5],\n",
      "    [6.0, 3, 79.0, 6],\n",
      "    [4.0, 4, 79.0, 7],\n",
      "    [4.0, 2, 79.0, 8],\n",
      "    [4.0, 2, 79.0, 8],\n",
      "    [6.0, 4, 79.0, 5],\n",
      "    [4.0, 3, 79.0, 8],\n",
      "    [4.0, 1, 79.0, 8],\n",
      "    [8.0, 4, 70.0, 3],\n",
      "    [4.0, 4, 70.0, 5],\n",
      "    [8.0, 5, 70.0, 2],\n",
      "    [6.0, 3, 70.0, 4],\n",
      "    [4.0, 2, 70.0, 7],\n",
      "    [4.0, 1, 70.0, 6],\n",
      "    [6.0, 3, 70.0, 5],\n",
      "    [8.0, 5, 70.0, 1],\n",
      "    [4.0, 2, 71.0, 7],\n",
      "    [4.0, 1, 71.0, 5],\n",
      "    [6.0, 4, 71.0, 4],\n",
      "    [8.0, 5, 71.0, 2],\n",
      "    [4.0, 2, 71.0, 5],\n",
      "    [4.0, 2, 71.0, 7],\n",
      "    [4.0, 1, 71.0, 8],\n",
      "    [4.0, 1, 71.0, 7],\n",
      "    [8.0, 5, 72.0, 2],\n",
      "    [8.0, 5, 72.0, 1],\n",
      "    [4.0, 2, 72.0, 7],\n",
      "    [6.0, 3, 73.0, 5],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [8.0, 5, 73.0, 1],\n",
      "    [4.0, 2, 73.0, 5],\n",
      "    [6.0, 2, 73.0, 5],\n",
      "    [4.0, 2, 73.0, 6],\n",
      "    [6.0, 3, 73.0, 5],\n",
      "    [4.0, 1, 74.0, 8],\n",
      "    [6.0, 5, 74.0, 3],\n",
      "    [4.0, 2, 74.0, 8],\n",
      "    [6.0, 5, 75.0, 3],\n",
      "    [4.0, 3, 75.0, 6],\n",
      "    [4.0, 2, 76.0, 6],\n",
      "    [4.0, 3, 76.0, 6],\n",
      "    [8.0, 5, 76.0, 2],\n",
      "    [4.0, 2, 76.0, 6],\n",
      "    [4.0, 1, 76.0, 8],\n",
      "    [6.0, 3, 76.0, 4],\n",
      "    [8.0, 5, 76.0, 1],\n",
      "    [4.0, 2, 77.0, 7],\n",
      "    [4.0, 1, 77.0, 8],\n",
      "    [6.0, 3, 77.0, 5],\n",
      "    [4.0, 1, 78.0, 8],\n",
      "    [4.0, 2, 78.0, 9],\n",
      "    [8.0, 4, 78.0, 4],\n",
      "    [4.0, 2, 78.0, 7],\n",
      "    [4.0, 2, 78.0, 5],\n",
      "    [4.0, 2, 79.0, 8],\n",
      "    [6.0, 3, 79.0, 7]]\n"
     ]
    }
   ],
   "source": [
    "def mpg_to_DOE(mpg):\n",
    "    if mpg >= 45:\n",
    "        y = 10\n",
    "    elif mpg >= 37:\n",
    "        y = 9\n",
    "    elif mpg >= 31:\n",
    "        y = 8\n",
    "    elif mpg >= 27:\n",
    "        y = 7\n",
    "    elif mpg >= 24:\n",
    "        y = 6\n",
    "    elif mpg >= 20:\n",
    "        y = 5\n",
    "    elif mpg >= 17:\n",
    "        y = 4\n",
    "    elif mpg >= 15:\n",
    "        y = 3\n",
    "    elif mpg >= 14:\n",
    "        y = 2\n",
    "    else:\n",
    "        y = 1\n",
    "    return y\n",
    "\n",
    "def weight_to_NHTSA(weight):\n",
    "    if weight >= 3500:\n",
    "        return 5\n",
    "    elif weight >= 3000:\n",
    "        return 4\n",
    "    elif weight >= 2500:\n",
    "        return 3\n",
    "    elif weight >= 2000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def discretize_auto_data(data):\n",
    "    discrete_data = []\n",
    "    for instance in data:\n",
    "        discrete = [instance[0], weight_to_NHTSA(instance[1]), instance[2], mpg_to_DOE(instance[3])]\n",
    "        discrete_data.append(discrete)\n",
    "    return discrete_data\n",
    "\n",
    "auto_data = discretize_auto_data(auto_data_c)\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(auto_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will rework our confusion matrix generator to format for the auto dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    ">>> pp.pprint(stuff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Stratefied 10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "Survived      no    yes    Total    Recognition (%)\n",
      "----------  ----  -----  -------  -----------------\n",
      "no             0    708      708                  0\n",
      "yes            0   1484     1484                100\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_tdidt(titanic_data, [0, 1, 2], [[4.0, 6.0, 8.0], [1, 2, 3, 4, 5], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
