{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 6 - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Decision Tree Classifier\n",
    "\n",
    "This Assignment is mainly focused on using decision trees to classify instances. As such, our first step is to create a function to generate these decision trees.\n",
    "\n",
    "First, we will need to import several modules later in this code, so we will do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import copy\n",
    "import random\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will need several helper functions that will be used in the `tdidt()` classifier.\n",
    "\n",
    "The first set are functions that are copied from the functions developed in class. We will pull `group_by()` and `get_column()` from previous repositories, as well as `partition_instances()` from the starter code for this assignment located in the `DecisionTreeFun` repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @Gina's Repo\n",
    "def groupBy(table, column_index, include_only_column_index=None):\n",
    "    # first identify unique values in the column\n",
    "    group_names = sorted(list(set(get_column(table, column_index))))\n",
    "\n",
    "    # now, we need a list of subtables\n",
    "    # each subtable corresponds to a value in group_names\n",
    "    # parallel arrays\n",
    "    groups = [[] for name in group_names]\n",
    "    for row in table:\n",
    "        # which group does it belong to?\n",
    "        group_by_value = row[column_index]\n",
    "        index = group_names.index(group_by_value)\n",
    "        if include_only_column_index is None:\n",
    "            groups[index].append(row.copy()) # note: shallow copy\n",
    "        else:\n",
    "            groups[index].append(row[include_only_column_index])\n",
    "\n",
    "    return group_names, groups\n",
    "\n",
    "# takes a table and a column index\n",
    "# returns a column at index where values are converted to numeric\n",
    "def get_column(table, column_index):\n",
    "    column = []\n",
    "    for item in table:\n",
    "        column.append(item[column_index])\n",
    "    return column\n",
    "\n",
    "def partition_instances(instances, att_index, att_domain):\n",
    "    # this is a group by att_domain, not by att_values in instances\n",
    "    partition = {}\n",
    "    for att_value in att_domain:\n",
    "        subinstances = []\n",
    "        for instance in instances:\n",
    "            # check if this instance has att_value at att_index\n",
    "            if instance[att_index] == att_value:\n",
    "                subinstances.append(instance)\n",
    "        partition[att_value] = subinstances\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need to develop several helper functions of our own to use. These are defined below:\n",
    "\n",
    "* `check_all_same_att()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances to check\n",
    "        * `index` - The index to query\n",
    "    * **Returns**:\n",
    "        * `True` if all instances in `instances` have the same value at `index`; `False` if else.\n",
    "* `check_all_same_class()`\n",
    "     * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances to check\n",
    "        * `class_index` - The index of the classifying variable\n",
    "    * **Returns**:\n",
    "        * `True` if all instances in `instances` have the same class; `False` if else.\n",
    "\n",
    "* `select_attribute()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances\n",
    "        * `att_indexes` - A list of valid indices to split on\n",
    "        * `class_index` - The index of the classifying attribute\n",
    "    * **Returns**:\n",
    "        * Uses the calculations for Entropy and Information Gain discussed in class to return the index attribute with the lowest entropy to be split on next\n",
    "* `handle_clash()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - A list of the current partitioned instances\n",
    "        * `class_index` - The index of the classifying attribute\n",
    "    * **Returns**:\n",
    "        * Uses majority voting to resolve clashes in the decision tree and create a Leaf Node with the most frequent class value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_all_same_att(instances, index):\n",
    "    base = instances[0][index]\n",
    "    for elem in instances:\n",
    "        if elem[index] != base:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_all_same_class(instances, class_index):\n",
    "    base = instances[0][class_index]\n",
    "    for elem in instances:\n",
    "        if elem[class_index] != base:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def select_attribute(instances, att_indexes, class_index):\n",
    "    Entropy_list = {}\n",
    "    for index in att_indexes:\n",
    "        E_new = 0\n",
    "        names, values = groupBy(instances, index)\n",
    "        for val in values:\n",
    "            ratios = {}\n",
    "            total = 0\n",
    "            for instance in val:\n",
    "                if instance[class_index] not in ratios:\n",
    "                    ratios[instance[class_index]] = 1\n",
    "                else:\n",
    "                    ratios[instance[class_index]] += 1\n",
    "                total += 1\n",
    "            E = 0\n",
    "            for ratio in ratios:\n",
    "                E += (ratios[ratio] / total) * math.log((ratios[ratio] / total), 2)\n",
    "            E_new += (total / len(instances)) * -E\n",
    "        Entropy_list[index] = E_new\n",
    "\n",
    "    min_i = att_indexes[0]\n",
    "    for index in att_indexes:\n",
    "        if Entropy_list[index] < Entropy_list[min_i]:\n",
    "            min_i = index\n",
    "    return min_i\n",
    "\n",
    "def handle_clash(instances, class_index):\n",
    "    votes = {}\n",
    "    for instance in instances:\n",
    "        if instance[class_index] not in votes:\n",
    "            votes[instance[class_index]] = 1\n",
    "        else:\n",
    "            votes[instance[class_index]] += 1\n",
    "    # Referenced from https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    sorted_x = sorted(votes.items(), reverse=True, key=operator.itemgetter(1))\n",
    "    return [\"Leaf\", sorted_x[0][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these classifiers, we can construct our `tdidt()` decision tree generator, as well as the `classify_tdidt()` classifier.\n",
    "\n",
    "* `tdidt()`\n",
    "    * **Parameters**:\n",
    "        * `instances` - The currently partitioned instances. On first recursive call, these are initialized as the entire dataset.\n",
    "        * `att_indexes` - A list of all valid indices to split on. On first recursive call, these are initialized as all attribute indices.\n",
    "        * `att_domains` - A list of all valid values for each attribute in the dataset.\n",
    "        * `class_index` - The index of the classifying attribute.\n",
    "    * **Returns**:\n",
    "        * A Decision Tree, represented using nested lists\n",
    "* `classify_tdidt()`\n",
    "    * **Parameters**:\n",
    "        * `tree` - A decision tree, generated by `tdidt()`\n",
    "        * `instance` - The unseen instance to classify\n",
    "    * **Returns**:\n",
    "        * A predicted classification using the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tdidt(instances, att_indexes, att_domains, class_index):\n",
    "    if check_all_same_class(instances, class_index):\n",
    "        return [\"Leaf\", instances[0][class_index]]\n",
    "    if att_indexes == []:\n",
    "        return handle_clash(instances, class_index)\n",
    "    index = select_attribute(instances, att_indexes, class_index)\n",
    "    new_indexes = att_indexes[:]\n",
    "    new_indexes.remove(index)\n",
    "    if check_all_same_att(instances, index):\n",
    "        return tdidt(instances, new_indexes, att_domains, class_index)\n",
    "    else:\n",
    "        tree = [\"Attribute\", index]\n",
    "        partitions = partition_instances(instances, index, att_domains[index])\n",
    "        for val in partitions:\n",
    "            if (partitions[val] == []):\n",
    "                return handle_clash(instances, class_index)\n",
    "            tree.append([\"Value\", val, tdidt(partitions[val], new_indexes, att_domains, class_index)])\n",
    "        return tree\n",
    "\n",
    "def classify_tdidt(tree, instance):\n",
    "    if tree[0] == 'Leaf':\n",
    "        return tree[1]\n",
    "    else:\n",
    "        i = 2\n",
    "        while (instance[tree[1]] != tree[i][1]):\n",
    "            i += 1\n",
    "        return classify_tdidt(tree[i][2], instance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our classifier, we will make use of the \"interview\" dataset provided in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "        [\"Senior\", \"Java\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"Java\", \"no\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"R\", \"yes\", \"yes\", \"False\"],\n",
    "        [\"Mid\", \"R\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"no\", \"no\", \"False\"],\n",
    "        [\"Senior\", \"R\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Senior\", \"Python\", \"yes\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Python\", \"no\", \"yes\", \"True\"],\n",
    "        [\"Mid\", \"Java\", \"yes\", \"no\", \"True\"],\n",
    "        [\"Junior\", \"Python\", \"no\", \"yes\", \"False\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first generate a decision tree using `tdidt()`, and then we will classify two instances: (\"Senior\", \"Java\", \"no\", \"yes\"), which exists in the dataset and should have the classification (\"False\"); and (\"Junior\", \"Java\", \"no\", \"no\"), which is not in the dataset but based on our in-class tree, should have the classification (\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tree = tdidt(table, [0, 1, 2, 3], [[\"Senior\", \"Mid\", \"Junior\"], [\"Java\", \"Python\", \"R\"], [\"no\", \"yes\"], [\"no\", \"yes\"]], 4)\n",
    "\n",
    "print(classify_tdidt(tree, [\"Senior\", \"Java\", \"no\", \"yes\"]))\n",
    "print(classify_tdidt(tree, [\"Junior\", \"Java\", \"no\", \"no\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Titanic Data\n",
    "\n",
    "This step asks us to use our classifier for the titanic dataset, and to test it using stratified 10-fold cross-validation.\n",
    "\n",
    "To do this, the first step will be to import the dataset and clean it as we have in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    f = open(filename, 'r')\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text\n",
    "\n",
    "def numerify_instance(instance):\n",
    "    new = []\n",
    "    for attribute in instance:\n",
    "        try:\n",
    "            new.append(float(attribute))\n",
    "        except:\n",
    "            new.append(attribute)\n",
    "    return new\n",
    "\n",
    "def create_dataset(data):\n",
    "    data_r = data.splitlines()\n",
    "    dataset_r = []\n",
    "    for line in data_r:\n",
    "        instance = line.split(',')\n",
    "        dataset_r.append(instance)\n",
    "    dataset = []\n",
    "    for instance in dataset_r:\n",
    "        newInstance = numerify_instance(instance)\n",
    "        dataset.append(newInstance)\n",
    "    return dataset\n",
    "\n",
    "def resolve_missing_values(data):\n",
    "    for i in range(10):\n",
    "        if i != 8:\n",
    "            sum_i = 0\n",
    "            count_i = 0\n",
    "            for instance in data:\n",
    "                if instance[i] != \"NA\":\n",
    "                    try:\n",
    "                        sum_i += instance[i]\n",
    "                        count_i += 1\n",
    "                    except:\n",
    "                        print(instance[i])\n",
    "            if count_i == 0:\n",
    "                continue\n",
    "            mean = sum_i / count_i\n",
    "            for instance in data:\n",
    "                if instance[i] == \"NA\":\n",
    "                    instance[i] = mean\n",
    "                    \n",
    "titanic_data = create_dataset(read_data(\"titanic_data.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also reuse the `create_cross_fold()` function from PA4 and PA5 for creating subsamples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_fold(data, n):\n",
    "    data_r = copy.deepcopy(data)\n",
    "    random.shuffle(data_r)\n",
    "    size = int(len(data) * 1/n) \n",
    "    start = 0\n",
    "    end = size\n",
    "    folds = []\n",
    "    for i in range(n-1):\n",
    "        folds.append(data[start:end])\n",
    "        start = end + 1\n",
    "        end += size + 1\n",
    "    folds.append(data[start:])\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do our cross-fold testing. We will write a function, `confusion_matrix_tdidt()` to both do the classification and generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_titanic(data, att_indexes, att_domains, class_index, n):\n",
    "    results = [[\"no\", 0, 0], [\"yes\", 0, 0]]\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        curr_tree = tdidt(train, att_indexes, att_domains, class_index)\n",
    "        predictions = [classify_tdidt(curr_tree, instance[:3]) for instance in test]\n",
    "        actual = [instance[3] for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            if actual[i] == \"no\":\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[0][1] += 1\n",
    "                else:\n",
    "                    results[0][2] += 1\n",
    "            else:\n",
    "                if predictions[i] == \"no\":\n",
    "                    results[1][1] += 1\n",
    "                else:\n",
    "                    results[1][2] += 1\n",
    "    for row in results:\n",
    "        row.append(sum(row[1:]))\n",
    "        if row[0] == \"no\":\n",
    "            row.append((row[1] / row[3]) * 100)\n",
    "        else:\n",
    "            row.append((row[2] / row[3]) * 100)\n",
    "    header = [\"Survived\", \"no\", \"yes\", \"Total\", \"Recognition (%)\"]\n",
    "    print(\"Decision Tree (Stratefied \", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can do our classification and generate a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Stratefied 10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "Survived      no    yes    Total    Recognition (%)\n",
      "----------  ----  -----  -------  -----------------\n",
      "no           268    440      708            37.8531\n",
      "yes           20   1464     1484            98.6523\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_titanic(titanic_data, [0, 1, 2], [[\"crew\", \"third\", \"second\", \"first\"], [\"adult\", \"child\"], [\"male\", \"female\"], [\"yes\", \"no\"]], 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that this classifier is amazing at predicting people that survived, but is terrible at predicting people that did not survive. Overall, this classifier is comparable to the one in PA5, which has a slightly better No prediction rate, but a slightly worse Yes rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MPG Classification\n",
    "\n",
    "For this step, we will use our `tdidt()` decision tree to classify mpg data from the auto dataset. As before, our first step is to import and clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_data_r = create_dataset(read_data(\"auto-data.txt\"))\n",
    "resolve_missing_values(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will restrict what attributes are included in our dataset. For the purpose of this assignment, we are concerned with the cylinders, weight, and model year attributes, as well as mpg as a classifier. We will copy `clean_auto_data()` from PA5 to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_auto_data(data):\n",
    "    cleaned_auto_data = []\n",
    "    for instance in data:\n",
    "        cleaned_instance = [instance[1], instance[4], instance[6], instance[0]]\n",
    "        cleaned_auto_data.append(cleaned_instance)\n",
    "    return cleaned_auto_data\n",
    "\n",
    "auto_data_c = clean_auto_data(auto_data_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as we did in PA5, we need to discretize the continuous attributes weight and cylinder. We will do this using the NHSTA vehicle size classification table and the DOE mpg classification rating, provided below.\n",
    "\n",
    "| Rating | MPG   |\n",
    "|--------|-----  |\n",
    "|   10   | ≥ 45  |\n",
    "|   9    | 37-44 |\n",
    "|   8    | 31-36 |\n",
    "|   7    | 27-30 |\n",
    "|   6    | 24-26 |\n",
    "|   5    | 20-23 |\n",
    "|   4    | 17-19 |\n",
    "|   3    | 15-16 |\n",
    "|   2    |   14  |\n",
    "|   1    | ≤ 13  |\n",
    "\n",
    "| Ranking |  Weight   |\n",
    "|---------|-----------|\n",
    "|    5    | ≥ 3500    |\n",
    "|    4    | 3000-3499 |\n",
    "|    3    | 2500-2999 |\n",
    "|    2    | 2000-2499 |\n",
    "|    1    | ≤ 1999    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpg_to_DOE(mpg):\n",
    "    if mpg >= 45:\n",
    "        y = 10\n",
    "    elif mpg >= 37:\n",
    "        y = 9\n",
    "    elif mpg >= 31:\n",
    "        y = 8\n",
    "    elif mpg >= 27:\n",
    "        y = 7\n",
    "    elif mpg >= 24:\n",
    "        y = 6\n",
    "    elif mpg >= 20:\n",
    "        y = 5\n",
    "    elif mpg >= 17:\n",
    "        y = 4\n",
    "    elif mpg >= 15:\n",
    "        y = 3\n",
    "    elif mpg >= 14:\n",
    "        y = 2\n",
    "    else:\n",
    "        y = 1\n",
    "    return y\n",
    "\n",
    "def weight_to_NHTSA(weight):\n",
    "    if weight >= 3500:\n",
    "        return 5\n",
    "    elif weight >= 3000:\n",
    "        return 4\n",
    "    elif weight >= 2500:\n",
    "        return 3\n",
    "    elif weight >= 2000:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "def discretize_auto_data(data):\n",
    "    discrete_data = []\n",
    "    for instance in data:\n",
    "        discrete = [instance[0], weight_to_NHTSA(instance[1]), instance[2], mpg_to_DOE(instance[3])]\n",
    "        discrete_data.append(discrete)\n",
    "    return discrete_data\n",
    "\n",
    "auto_data = discretize_auto_data(auto_data_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will rework our confusion matrix generator to format for the auto dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_auto(data, att_indexes, att_domains, class_index, n):\n",
    "    results = [[\"1\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"2\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "               [\"3\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"4\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"5\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"6\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"7\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"8\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"9\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], \n",
    "               [\"10\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "    folds = create_cross_fold(data, n)\n",
    "    for i in range(n):\n",
    "        test = folds[i]\n",
    "        train = []\n",
    "        for x in range(n):\n",
    "            if x == i:\n",
    "                continue\n",
    "            train += folds[x]\n",
    "        curr_tree = tdidt(train, att_indexes, att_domains, class_index)\n",
    "        predictions = [classify_tdidt(curr_tree, instance[:3]) for instance in test]\n",
    "        actual = [instance[3] for instance in test]\n",
    "        for i in range(len(actual)):\n",
    "            y = actual[i]\n",
    "            x = predictions[x]\n",
    "            results[y-1][x] += 1\n",
    "    i = 1\n",
    "    for row in results:\n",
    "        row.append(sum(row[1:]))\n",
    "        if row[11] == 0:\n",
    "            row.append(\"NaN\")\n",
    "        else:\n",
    "            row.append((row[i] / row[11]) * 100)\n",
    "        i += 1\n",
    "    header = [\"MPG Rating\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Total\", \"Recognition (%)\"]\n",
    "    print(\"Decision Tree (Stratefied \", n, \"-Fold Cross Validation Results):\", sep=\"\")\n",
    "    print(\"===============================================================\")\n",
    "    print(tabulate(results, headers=header))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Stratefied 10-Fold Cross Validation Results):\n",
      "===============================================================\n",
      "  MPG Rating    1    2    3    4    5    6    7    8    9    10    Total    Recognition (%)\n",
      "------------  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  -----------------\n",
      "           1   14    1    0    1    6    3    4    3    0     0       32           43.75\n",
      "           2   13    0    0    1    2    0    1    1    2     0       20            0\n",
      "           3   13    0    1   13    1    5    3    0    2     0       38            2.63158\n",
      "           4   15    2    0   16    3    6    9    2    1     0       54           29.6296\n",
      "           5   16    0    1   21    1    6   10    5    3     0       63            1.5873\n",
      "           6   15    3    1    8    3    3    5    3    0     0       41            7.31707\n",
      "           7    5    1    3    8    0    7    5    0    4     0       33           15.1515\n",
      "           8    1    0    3    8    1    1    2    3    3     0       22           13.6364\n",
      "           9    0    0    0    2    1    0    0    0    0     0        3            0\n",
      "          10    0    0    0    0    0    0    0    0    0     0        0          nan\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_auto(auto_data, [0, 1, 2], [[4.0, 6.0, 8.0], [1, 2, 3, 4, 5], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this decision tree is all over the board with its classifications. Some, like predicting MPG class 1, are fairly good, while others like class 2 and 9 are absolutely awful. This makes it tough to compare directly to the classifier in PA5, but overall I would say that it is slightly worse, as only one classification gets above a 40% prediction rate, with the average being closer to 15-20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Rule Generation\n",
    "\n",
    "For this step, we will attempt to visualize the rules that our decision tree is actually using to classify instances. To do this, we will need to define a rule inference funtion, which we will call `print_rules()`. This will be a recursive function which will loop through the nested list structure of the tree we generate and will print out the underlying rules that define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rules(tree, attributes, string):\n",
    "    if tree[0] == \"Attribute\":\n",
    "        if string == \"\":\n",
    "            string += \"IF \" + str(attributes[tree[1]]) + \" == \"\n",
    "        else:\n",
    "            string += \" AND \" + str(attributes[tree[1]]) + \" == \"\n",
    "        for value in tree[2:]:\n",
    "            valString = string + str(value[1])\n",
    "            print_rules(value[2], attributes, valString)\n",
    "    elif tree[0] == \"Leaf\":\n",
    "        string += \" THEN class == \" + str(tree[1])\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create overall data trees for the titanic and auto datasets, and see what rules the trees generate.\n",
    "\n",
    "First, we will generate a tree for the titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF Sex == male AND Class == crew AND Age == adult THEN class == no\n",
      "IF Sex == male AND Class == crew AND Age == child THEN class == no\n",
      "IF Sex == male AND Class == third THEN class == no\n",
      "IF Sex == male AND Class == second AND Age == adult THEN class == yes\n",
      "IF Sex == male AND Class == second AND Age == child THEN class == yes\n",
      "IF Sex == male AND Class == first AND Age == adult THEN class == no\n",
      "IF Sex == male AND Class == first AND Age == child THEN class == no\n",
      "IF Sex == female AND Class == crew AND Age == adult THEN class == yes\n",
      "IF Sex == female AND Class == crew AND Age == child THEN class == no\n",
      "IF Sex == female AND Class == third THEN class == yes\n",
      "IF Sex == female AND Class == second AND Age == adult THEN class == yes\n",
      "IF Sex == female AND Class == second AND Age == child THEN class == yes\n",
      "IF Sex == female AND Class == first AND Age == adult THEN class == yes\n",
      "IF Sex == female AND Class == first AND Age == child THEN class == no\n"
     ]
    }
   ],
   "source": [
    "titanic_tree = tdidt(titanic_data, [0, 1, 2], [[\"crew\", \"third\", \"second\", \"first\"], [\"adult\", \"child\"], [\"male\", \"female\"], [\"yes\", \"no\"]], 3)\n",
    "print_rules(titanic_tree, [\"Class\", \"Age\", \"Sex\", \"Survived\"], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the auto dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IF Weight Class == 1 THEN class == 8\n",
      "IF Weight Class == 2 AND Model Year == 70 THEN class == 6\n",
      "IF Weight Class == 2 AND Model Year == 71 THEN class == 7\n",
      "IF Weight Class == 2 AND Model Year == 72 THEN class == 5\n",
      "IF Weight Class == 2 AND Model Year == 73 THEN class == 5\n",
      "IF Weight Class == 2 AND Model Year == 74 THEN class == 6\n",
      "IF Weight Class == 2 AND Model Year == 75 THEN class == 7\n",
      "IF Weight Class == 2 AND Model Year == 76 THEN class == 7\n",
      "IF Weight Class == 2 AND Model Year == 77 THEN class == 7\n",
      "IF Weight Class == 2 AND Model Year == 78 THEN class == 7\n",
      "IF Weight Class == 2 AND Model Year == 79 THEN class == 8\n",
      "IF Weight Class == 3 AND Model Year == 70 THEN class == 5\n",
      "IF Weight Class == 3 AND Model Year == 71 THEN class == 4\n",
      "IF Weight Class == 3 AND Model Year == 72 THEN class == 5\n",
      "IF Weight Class == 3 AND Model Year == 73 THEN class == 4\n",
      "IF Weight Class == 3 AND Model Year == 74 THEN class == 4\n",
      "IF Weight Class == 3 AND Model Year == 75 THEN class == 5\n",
      "IF Weight Class == 3 AND Model Year == 76 THEN class == 6\n",
      "IF Weight Class == 3 AND Model Year == 77 THEN class == 5\n",
      "IF Weight Class == 3 AND Model Year == 78 THEN class == 5\n",
      "IF Weight Class == 3 AND Model Year == 79 THEN class == 7\n",
      "IF Weight Class == 4 THEN class == 4\n",
      "IF Weight Class == 5 AND Model Year == 70 THEN class == 3\n",
      "IF Weight Class == 5 AND Model Year == 71 THEN class == 2\n",
      "IF Weight Class == 5 AND Model Year == 72 THEN class == 1\n",
      "IF Weight Class == 5 AND Model Year == 73 THEN class == 1\n",
      "IF Weight Class == 5 AND Model Year == 74 THEN class == 3\n",
      "IF Weight Class == 5 AND Model Year == 75 THEN class == 3\n",
      "IF Weight Class == 5 AND Model Year == 76 THEN class == 3\n",
      "IF Weight Class == 5 AND Model Year == 77 THEN class == 3\n",
      "IF Weight Class == 5 AND Model Year == 78 THEN class == 4\n",
      "IF Weight Class == 5 AND Model Year == 79 THEN class == 4\n"
     ]
    }
   ],
   "source": [
    "auto_tree = tdidt(auto_data, [0, 1, 2], [[4.0, 6.0, 8.0], [1, 2, 3, 4, 5], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], 3)\n",
    "print_rules(auto_tree, [\"Cylinders\", \"Weight Class\", \"Model Year\", \"MPG\"], \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that, while the titanic ruleset had varied and interesting rule splits, the auto dataset strictly splits on Weight Class and then Model Year for every single rule, save for the two that jump straight to a prediction after the first split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
